{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import modules.callbacks\n",
    "import PIL\n",
    "import base64\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    #From text, count how many . ! ? there are\n",
    "    #If . ! ? are followed by a space,double mark or a new line, then it is a sentence\n",
    "    #If . ! ? in the end of the text, then it is a sentence\n",
    "    return len(re.findall(r'[\\.\\!\\?][\\s\\â€\\'\\n]',text)) + len(re.findall(r'[\\.\\!\\?]$',text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Model (Help Txt2Img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:50:54.773112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-26 14:50:56.429590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/wsl/lib::/home/karuniaperjuangan/anaconda3/lib::/usr/local/cuda-11.2/targets/x86_64-linux/lib::\n",
      "2023-03-26 14:50:56.431363: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/wsl/lib::/home/karuniaperjuangan/anaconda3/lib::/usr/local/cuda-11.2/targets/x86_64-linux/lib::\n",
      "2023-03-26 14:50:56.431372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "id_to_en_pipeline = pipeline(\"translation_id_to_en\", model=\"Helsinki-NLP/opus-mt-id-en\", tokenizer=\"Helsinki-NLP/opus-mt-id-en\", device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_torch_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/karuniaperjuangan/anaconda3/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/karuniaperjuangan/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path_or_name = \"/home/karuniaperjuangan/AI-Project/text-generation-webui/models/gpt2-medium-indonesian-story\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path_or_name,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\"\n",
    "                                             ).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name,truncation_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate_text(init_text,max_sentences=0,max_tokens=512,hidden_texts=[]):\n",
    "    \"\"\"\n",
    "    init_text: initial text that will be used to generate text\n",
    "    max_sentences: maximum number of sentences to generate\n",
    "    max_tokens: maximum number of tokens to generate\n",
    "    hidden_sentence: list of string that will be hidden from the generated text (example: initial text and unfinished sentences)\n",
    "    \"\"\"\n",
    "    sentence = init_text\n",
    "    inputs_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda()\n",
    "    continue_generate = True\n",
    "    while continue_generate:\n",
    "        inputs_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda()\n",
    "        generate_params = {\n",
    "        \"max_new_tokens\": 8,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"do_sample\": True,\n",
    "        }\n",
    "        generate_params[\"input_ids\"] = inputs_ids\n",
    "        outputs = model.generate(**generate_params)\n",
    "        sentence = tokenizer.decode(outputs[0])\n",
    "        inputs_ids = outputs[0]\n",
    "        #print(count_sentences(sentence)) \n",
    "        outputted_sentences = sentence\n",
    "        for hidden in hidden_texts:\n",
    "            outputted_sentences = outputted_sentences.replace(hidden,\"\")\n",
    "        yield outputted_sentences\n",
    "            \n",
    "        if count_sentences(sentence) >= max_sentences or ( len(inputs_ids)-len(tokenizer.encode(init_text)) ) > max_tokens or tokenizer.eos_token_id in inputs_ids:\n",
    "            continue_generate = False\n",
    "    #Clear text after last ,.\",!\",?\",.,?,!\n",
    "    deleted_text = re.split(r\"[\\.\\!\\?]\",sentence)[-1]\n",
    "\n",
    "    outputted_sentences = sentence.replace(deleted_text,\"\")\n",
    "    for hidden in hidden_texts:\n",
    "        outputted_sentences = outputted_sentences.replace(hidden,\"\")\n",
    "    yield outputted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_sequence = []\n",
    "\n",
    "def generate_story_and_choices(beginning_text, selected_choice=\"\", num_choices=3, story_beginning=True):\n",
    "    clear_torch_cache()\n",
    "    \"\"\"\n",
    "    beginning_text: text that will be used to generate story (if in beginning is the story, else the choice of previous story)\n",
    "    num_choices: number of choices to generate\n",
    "    story_beginning: True if the beginning_text is the story, else False\n",
    "    \"\"\"\n",
    "    global story_sequence\n",
    "    if story_beginning:\n",
    "        story_sequence = []\n",
    "        story = beginning_text\n",
    "    else:\n",
    "        story = \"\"\n",
    "        for item in story_sequence:\n",
    "            story += item\n",
    "    pregenerated_length = len(story)\n",
    "    \n",
    "    story += selected_choice\n",
    "\n",
    "    \n",
    "    choices = [\"...\" for i in range(num_choices)]\n",
    "\n",
    "    for text in stream_generate_text(story,max_sentences=5,max_tokens=512):\n",
    "        story = text\n",
    "        # Di awal, juga tampilkan awalan cerita yang dimasukkan pengguna\n",
    "        yield story[pregenerated_length:] if not story_beginning else story, *choices\n",
    "    \n",
    "    #clean story from html tag\n",
    "    story = re.sub(r\"<[^>]*>\",\"\",story)\n",
    "    \n",
    "    story_sequence.append(story[pregenerated_length:] if not story_beginning else story)\n",
    "    #Generate choices\n",
    "    max_sentences = 2+count_sentences(story)\n",
    "    for i in range(num_choices):\n",
    "        choices[i] = \"\"\n",
    "        for text in stream_generate_text(story,max_sentences=max_sentences,max_tokens=512):\n",
    "            choices[i] = text[len(story):]\n",
    "            yield story[pregenerated_length:] if not story_beginning else story, *choices\n",
    "        #clean choices from html tag\n",
    "        choices[i] = re.sub(r\"<[^>]*>\",\"\",choices[i])\n",
    "        \n",
    "def select_choice(choice):\n",
    "    global story_sequence\n",
    "    full_story = \"\"\n",
    "    for item in story_sequence:\n",
    "        full_story += item\n",
    "    for iter in generate_story_and_choices(\"\",selected_choice=choice,story_beginning=False):\n",
    "        yield iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "blank_image = Image.new('RGB', (768, 768), (255,255,255))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_api = \"http://127.0.0.1:3760/sdapi/v1/txt2img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_illustration(story=\"\",magic_prompt=\"\",negative_prompt=\"((nsfw)),ugly,deformed,jpeg artifacts, signature, watermark, username, artist name\"):\n",
    "    \"\"\"\n",
    "    story: story to be illustrated\n",
    "    magic_prompt: magic prompt to be used to generate illustration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(url_api, json={\n",
    "        \"prompt\": f'\"{story}\", {magic_prompt} ',\n",
    "        \"seed\": -1,\n",
    "        \"steps\": 20,\n",
    "        \"cfg_scale\": 7,\n",
    "        \"width\": 768,\n",
    "        \"height\": 768,\n",
    "        \"negative_prompt\": negative_prompt\n",
    "        }).json()\n",
    "        image = Image.open(io.BytesIO(base64.b64decode(response[\"images\"][0])))\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(\"Error generating illustration:\",e)\n",
    "        return blank_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustration_from_story(story,magic_prompt=\"illustration, kid story, samdoesart style\"):\n",
    "    #Take the first 2 sentences of the story\n",
    "    story_sentences = re.split(r\"[\\.\\!\\?]\",story)\n",
    "    story_sentences = \"\".join(story_sentences[:2])\n",
    "    #Translate prompt from Indonesian to English\n",
    "    prompt = id_to_en_pipeline(story_sentences)[0][\"translation_text\"]\n",
    "    #clean prompt from html tag\n",
    "    prompt = re.sub(r\"<[^>]*>\",\"\",prompt)\n",
    "    #Generate illustration\n",
    "    image = create_illustration(story=prompt,magic_prompt=magic_prompt)\n",
    "    return [image]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karuniaperjuangan/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:201: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css='./style.css') as blocks:\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            text = gr.Textbox(label=\"Text\", value=\"Pada zaman dahulu,\")\n",
    "            button = gr.Button(value=\"Generate\")\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                output = gr.Markdown(value=\"This is a test\")\n",
    "                #image = gr.Gallery(value=[blank_image])\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row(elem_classes=\"choicerow\"):\n",
    "                        button_1 = gr.Button(value=\"Select\", elem_classes=\"choicebutton\")\n",
    "                        choice_1 = gr.Markdown(value=\"This is a choice 1\",elem_classes=\"choice-label\")\n",
    "                    with gr.Row(elem_classes=\"choicerow\"):\n",
    "                        button_2 = gr.Button(value=\"Select\", elem_classes=\"choicebutton\")\n",
    "                        choice_2 = gr.Markdown(value=\"This is a choice 1\",elem_classes=\"choice-label\")\n",
    "                    with gr.Row(elem_classes=\"choicerow\"):\n",
    "                        button_3 = gr.Button(value=\"Select\", elem_classes=\"choicebutton\")\n",
    "                        choice_3 = gr.Markdown(value=\"This is a choice 1\",elem_classes=\"choice-label\")\n",
    "                images = gr.Gallery(value=[blank_image])    \n",
    "            with gr.Row():\n",
    "                magic_prompt = gr.Textbox(label=\"Magic Prompt\", value=\"Detail, Sharp focus, dramatic, RAW photo, 8k uhd, film grain\")\n",
    "                \n",
    "                \n",
    "\n",
    "        with gr.Row():\n",
    "            pass\n",
    "    button.click(generate_story_and_choices,[text],[output,choice_1,choice_2,choice_3]).then(illustration_from_story,inputs=[output,magic_prompt],outputs=[images])\n",
    "    button_1.click(select_choice,[choice_1],[output,choice_1,choice_2,choice_3]).then(illustration_from_story,inputs=[output,magic_prompt],outputs=[images])\n",
    "    button_2.click(select_choice,[choice_2],[output,choice_1,choice_2,choice_3]).then(illustration_from_story,inputs=[output,magic_prompt],outputs=[images])\n",
    "    button_3.click(select_choice,[choice_3],[output,choice_1,choice_2,choice_3]).then(illustration_from_story,inputs=[output,magic_prompt],outputs=[images])\n",
    "        \n",
    "        \n",
    "blocks.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
